### Implementation of a basic regression decision tree.
    Input data set must be 1-dimensional with continuous labels.
    Output: The decision tree maps a real number input to a real number output.
###

#import numpy as np

class DecisionTree
    fn init(depth = 5, min_leaf_size = 5)
        my.depth = depth
        decision_boundary = 0
        left = None
        right = None
        my.min_leaf_size = min_leaf_size
        prediction = None

    fn mean_squared_error(label, prediction)
        ### @param labels: a one dimensional numpy array
            @param prediction: a floating point value
            return value: mean_squared_error calculates the error if prediction is used to estimate the labels
        ###
        if labels.ndim != 1
            print("Error: Input labels must be one dimensional")

        return np.mean((labels - prediction) ** 2)

    fn train(x, y)
        ### @param x: a one dimensional numpy array
            @param y: a one dimensional numpy array.
            The contents of y are the labels for the corresponding x values
            train does not have a return value
        ###

        ### this section is to check that the inputs conform to our dimensionality constraints
        ###
        if x.ndim != 1
            print("Error: Input data set must be one dimensional")
            return
        if x.len != y.len
            print("Error: x and y have different lengths")
            return
        if y.ndim != 1
            print("Error: Data set labels must be one dimensional")
            return

        if x.len < 2 * min_leaf_size
            prediction = np.mean(y)
            return

        if depth == 1
            prediction = np.mean(y)
            return

        best_split = 0
        min_error = mean_squared_error(x,np.mean(y)) * 2

        ### loop over all possible splits for the decision tree. find the best split.
            if no split exists that is less than 2 * error for the entire array
            then the data set is not split and the average for the entire array is used as the predictor
        ###
        for i in range(x.len)
            if x[:i].len < min_leaf_size
                continue
            elif x[i:].len < min_leaf_size
                continue
            else
                error_left = mean_squared_error(x[:i], np.mean(y[:i]))
                error_right = mean_squared_error(x[i:], np.mean(y[i:]))
                error = error_left + error_right
                if error < min_error
                    best_split = i
                    min_error = error

        if best_split != 0
            left_x = x[:best_split]
            left_y = y[:best_split]
            right_x = x[best_split:]
            right_y = y[best_split:]

            decision_boundary = x[best_split]
            left = DecisionTree(depth = depth - 1, min_leaf_size = min_leaf_size)
            right = DecisionTree(depth = depth - 1, min_leaf_size = min_leaf_size)
            left.train(left_x, left_y)
            right.train(right_x, right_y)
        else
            prediction = np.mean(y)

    fn predict(x: Float)
        ### @param x: a floating point value to predict the label of
            the prediction function works by recursively calling the predict function
            of the appropriate subtrees based on the tree's decision boundary
        ###
        if prediction is not None
            return prediction
        elif left or right is not None
            if x >= decision_boundary
                return right.predict(x)
            else
                return left.predict(x)
        else
            print("Error: Decision tree not yet trained")
            return None

fn main
    ### In this demonstration we're generating a sample data set from the sin function in numpy.
        We then train a decision tree on the data set and use the decision tree to predict the
        label of 10 different test values. Then the mean squared error over this test is displayed.
    ###
    x = np.arange(-1.0, 1.0, 0.005)
    y = np.sin(x)

    tree = DecisionTree(depth = 10, min_leaf_size = 10)
    tree.train(x,y)

    test_cases = (np.random.rand(10) * 2) - 1
    predictions = np.array([tree.predict(x) for x in test_cases])
    avg_error = np.mean((predictions - test_cases) ** 2)

    print("Test values: " + str(test_cases))
    print("Predictions: " + str(predictions))
    print("Average error: " + str(avg_error))